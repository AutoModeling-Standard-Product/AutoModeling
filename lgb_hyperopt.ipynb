{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.utils.analysis import *\n",
    "from main.utils.data_augmentation import *\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_feas = pd.read_csv(\"data/feas_x/tx_fined_feas.csv\")\n",
    "supply_feas = pd.read_csv(\"data/feas_x/tx_supplyment_feas.csv\")\n",
    "feas = set(list(fine_feas['0']) + list(supply_feas['0']) + ['matchingid', 'org', 'target', 'apply_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'data_pth':'data/tx_unfilterdata.csv',\n",
    "    'date_colName': 'apply_date',\n",
    "    'y_colName': 'target',\n",
    "    'org_colName':'org',\n",
    "    'data_encode':'utf-8',\n",
    "    'key_colNames' : ['matchingid'],\n",
    "    'use_cols':feas\n",
    "}\n",
    "data = get_dataset(**params)\n",
    "\n",
    "params = {\n",
    "    'data': data,\n",
    "    'minYmBadsample': 5,\n",
    "    'minYmSample': 0\n",
    "}\n",
    "data = drop_abnormal_ym(**params)\n",
    "\n",
    "data.replace({-1111:np.nan, -999:np.nan, -1:np.nan}, inplace=True)\n",
    "\n",
    "oos_data = data[data.new_org.isin(['202412050001_光大信用卡_分期', '202408260001_上海银行', '202412040001_中原银行',\n",
    "                  '202412040002_中原银行', '202406140001_分期乐_欺诈', '202403280002_长银消金_唯品会',\n",
    "                 '202504210002_华通_360', '202408130001_洋钱罐', '202411040001_汇登数字', '202412090001_广州农商行'])]\n",
    "\n",
    "data = data[~data.new_org.isin(['202412050001_光大信用卡_分期', '202408260001_上海银行', '202412040001_中原银行',\n",
    "                  '202412040002_中原银行', '202406140001_分期乐_欺诈', '202403280002_长银消金_唯品会',\n",
    "                 '202504210002_华通_360', '202408130001_洋钱罐', '202411040001_汇登数字', '202412090001_广州农商行'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# 内存使用\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"总内存: {mem.total / (1024**3):.1f} GB\")\n",
    "print(f\"已用内存: {mem.used / (1024**3):.1f} GB\")\n",
    "\n",
    "# CPU使用\n",
    "print(f\"CPU核心数: {psutil.cpu_count(logical=True)}\")\n",
    "print(f\"当前CPU占用: {psutil.cpu_percent()}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取建模数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_feas = pd.read_csv(\"data/feas_x/tx_fined_feas.csv\")\n",
    "supply_feas = pd.read_csv(\"data/feas_x/tx_supplyment_feas.csv\")\n",
    "feas = set(list(fine_feas['0']) + list(supply_feas['0']) + ['matchingid', 'org', 'target', 'apply_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'data_pth':'data/tx_unfilterdata.csv',\n",
    "    'date_colName': 'apply_date',\n",
    "    'y_colName': 'target',\n",
    "    'org_colName':'org',\n",
    "    'data_encode':'utf-8',\n",
    "    'key_colNames' : ['matchingid'],\n",
    "    'use_cols':feas}\n",
    "data = get_dataset(**params)\n",
    "\n",
    "params = {\n",
    "    'data': data,\n",
    "    'minYmBadsample': 10,\n",
    "    'minYmSample': 0\n",
    "}\n",
    "data = drop_abnormal_ym(**params)\n",
    "\n",
    "data.replace({-1111:np.nan, -999:np.nan, -1:np.nan}, inplace=True)\n",
    "\n",
    "data = data[~data.new_org.isin(['202412050001_光大信用卡_分期', '202408260001_上海银行', '202412040001_中原银行',\n",
    "                  '202412040002_中原银行', '202406140001_分期乐_欺诈', '202403280002_长银消金_唯品会',\n",
    "                 '202504210002_华通_360', '202408130001_洋钱罐', '202411040001_汇登数字', '202412090001_广州农商行'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperOptLGB(object):\n",
    "    '''\n",
    "        init: data, params, fobj, weights, max_iteration\n",
    "        \n",
    "        excuting func: tpesearch_params()\n",
    "        \n",
    "        return: train log, trails\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        self.data = kwargs.get('data')\n",
    "        self.params = kwargs.get('params')\n",
    "        self.fobj = kwargs.get('fobj')\n",
    "        self.max_iterations = kwargs.get('max_iterations')\n",
    "        self.record_train_process = kwargs.get(\"record_train_process\")\n",
    "        self.auc_threshold = kwargs.get('auc_threshold')\n",
    "        self.randn = kwargs.get('randn')\n",
    "        self.X_tr, self.X_val, self.y_tr, self.y_val, self.tr_orgidx, self.val_orgidx = self.split_data(self.data)\n",
    "        self.trails = Trials()\n",
    "    \n",
    "    # 输入数据集，返回8:2切分的训练验证集 & 字典形式储存的各机构在训练集验证集的索引，确保输入的数据集有new_org, new_target列\n",
    "    def split_data(self, data):\n",
    "        \n",
    "        # 确定X，判定条件不是object类型且不是Y列\n",
    "        feas = [v for v in data.columns if data[v].dtype!='O' and v!='new_target']\n",
    "        \n",
    "        # tr_orgidx存储训练集各个机构索引，tr_idx存储训练集全部索引, val为验证集同样做法\n",
    "        tr_orgidx, val_orgidx, tr_idx, val_idx = {}, {}, [], []\n",
    "        \n",
    "        # 分层抽样\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, random_state=self.randn, train_size=0.8)\n",
    "        \n",
    "        for org in data.new_org.unique():\n",
    "            tmp_data = data[data.new_org==org].copy()\n",
    "            org_index = tmp_data.index\n",
    "            \n",
    "            # 每个机构下分层抽样, 注意不要使用相对索引否则会造成取值错误, 这里使用了绝对索引\n",
    "            for idx_tr, idx_val in splitter.split(tmp_data[feas], tmp_data['new_target']):\n",
    "                tr_orgidx[org] = list(org_index[idx_tr])\n",
    "                val_orgidx[org] = list(org_index[idx_val])\n",
    "                val_idx += list(org_index[idx_val])\n",
    "                tr_idx += list(org_index[idx_tr])\n",
    "        \n",
    "        # 分出训练、验证集\n",
    "        data_tr, data_val = data.loc[tr_idx, ], data.loc[val_idx, ]\n",
    "        X_tr, X_val, y_tr, y_val = data_tr[feas], data_val[feas], data_tr['new_target'], data_val['new_target']\n",
    "        return X_tr, X_val, y_tr, y_val, tr_orgidx, val_orgidx\n",
    "    \n",
    "    def _get_lift(self, y, pred, k):\n",
    "        \n",
    "        n_top = int(len(y) * k)\n",
    "        top_indices = pd.Series(pred, index=y.index).sort_values(ascending=False).head(n_top).index\n",
    "        \n",
    "        return y[top_indices].mean() / y.mean()\n",
    "            \n",
    "    ## 自定义feval评价函数，闭包方式，输入加权&不加权ks auc等, lightgbm==2.3.0时每次迭代都会强制评估无法跳过, 每次评估约为0.2-0.8秒均值为0.3秒\n",
    "    def weighted_metric(self, weights):\n",
    "        \n",
    "        def _weighted_metric(pred_, data):\n",
    "            y = data.get_label()\n",
    "            pred = 1 / (1 + np.exp(-pred_))\n",
    "            \n",
    "            ## 比较输入的数据长度和w_val_, w_tr_, w_oos_判断使用哪个权重\n",
    "            if len(y) == len(weights.get('train')):\n",
    "                w = weights.get('train')\n",
    "            elif len(y) == len(weights.get('val')):\n",
    "                w = weights.get('val')\n",
    "            else:\n",
    "                w = weights.get('oos')\n",
    "                \n",
    "            auc_w= roc_auc_score(y, pred, sample_weight=w)\n",
    "            fpr, tpr, _ = roc_curve(y, pred, sample_weight=w)\n",
    "            \n",
    "            ks = toad.metrics.KS(pred, y)\n",
    "            ks_w = max(tpr - fpr)\n",
    "            \n",
    "            lift10, lift5 = self._get_lift(y, pred, 0.1), self._get_lift(y, pred, 0.05)\n",
    "            \n",
    "            return [\n",
    "                    ('auc_w',auc_w,True),\n",
    "                    ('ks_w',ks_w,True), ('ks',ks,True),\n",
    "                    ('5%lift',lift5,True), ('10%lift',lift10,True)\n",
    "                ]\n",
    "        \n",
    "        return _weighted_metric\n",
    "    \n",
    "    ## 根据model和输入数据返回带权重&不带权重计算下的auc ks，替代weighted_metric用，只计算最后一次\n",
    "    def single_weighted_metric(self, model, X, y, w):\n",
    "        pred = model.predict(X)\n",
    "        pred = pd.Series(pred, index=X.index)\n",
    "        auc_w, auc = roc_auc_score(y, pred, sample_weight=w), roc_auc_score(y, pred)\n",
    "        fpr, tpr, _ = roc_curve(y, pred, sample_weight=w)\n",
    "        \n",
    "        ks = toad.metrics.KS(pred, y)\n",
    "        ks_w = max(tpr - fpr)\n",
    "        \n",
    "        lift10, lift5 = self._get_lift(y, pred, 0.1), self._get_lift(y, pred, 0.05)\n",
    "        \n",
    "        return auc_w, auc, ks_w, ks, lift5, lift10\n",
    "    \n",
    "    ## 提取训练日志中的最后一次迭代结果\n",
    "    def extract_evalresult(self, records):\n",
    "        \n",
    "        simpler_records = []\n",
    "        \n",
    "        for org in records.org.unique():\n",
    "            tmp_record = records[records.org==org].copy()\n",
    "            \n",
    "            simpler_records.append({\n",
    "                'org': org, 'tr_auc':tmp_record.train.auc[-1], 'tr_auc_w':tmp_record.train.auc_w[-1],\n",
    "                'tr_ks':tmp_record.train.ks[-1], 'tr_ks_w': tmp_record.train.ks_w[-1],\n",
    "                'tr_5lift':tmp_record.train['5%lift'][-1], 'tr_10lift':tmp_record.train['10%lift'][-1],\n",
    "                'val_auc':tmp_record.val.auc[-1], 'val_auc_w':tmp_record.val.auc_w[-1],\n",
    "                'val_ks':tmp_record.val.ks[-1], 'val_ks_w':tmp_record.val.ks_w[-1],\n",
    "                'val_5lift':tmp_record.val['5%lift'][-1], 'val_10lift':tmp_record.val['10%lift'][-1],\n",
    "                'oos_auc': tmp_record.oos.auc[-1], 'oos_ks':tmp_record.oos.ks[-1],\n",
    "                'oos_5lift':tmp_record.oos['5%lift'][-1], 'oos_10lift':tmp_record.oos['10%lift'][-1]\n",
    "                })\n",
    "        \n",
    "        simpler_records = pd.DataFrame(simpler_records)\n",
    "        \n",
    "        return simpler_records\n",
    "    \n",
    "    # 每组参数下分机构cv训练, 返回每个机构做oos下的train val oos ks, oos ks\n",
    "    def train_epoch_(self, org, param):\n",
    "        \n",
    "        broadcast_with_tar = param.get('broadcast_with_tar')\n",
    "        balanced_badrate = param.get(\"balanced_badrate\")\n",
    "        param['num_leaves'] = 2**param.get('max_depth')-1\n",
    "        \n",
    "        ## 根据超参数坏样率得出权重weight\n",
    "        weight_tr = re_weight_by_org(self.y_tr, self.tr_orgidx, 0.5, broadcast_with_tar, balanced_badrate)\n",
    "        weight_val = re_weight_by_org(self.y_val, self.val_orgidx, 0.5, broadcast_with_tar, balanced_badrate)\n",
    "        weight = pd.concat([weight_tr, weight_val], axis=0)\n",
    "        \n",
    "        tr_idxs, val_idxs = set(self.X_tr.index), set(self.X_val.index)\n",
    "        tr_idx, val_idx = self.tr_orgidx.get(org), self.val_orgidx.get(org)\n",
    "        \n",
    "        # 除去当前org选出训练验证集\n",
    "        X_tr_, y_tr_ = self.X_tr.loc[list(tr_idxs-set(tr_idx)), ], self.y_tr.loc[list(tr_idxs-set(tr_idx)), ]\n",
    "        X_val_, y_val_ = self.X_val.loc[list(val_idxs-set(val_idx)), ], self.y_val.loc[list(val_idxs-set(val_idx)), ]\n",
    "        w_tr_, w_val_ = weight.loc[list(tr_idxs-set(tr_idx)), ], weight.loc[list(val_idxs-set(val_idx)), ]\n",
    "#         w_tr_ = pd.Series(np.ones(X_tr_.shape[0]), index=X_tr_.index)\n",
    "#         w_val_ = pd.Series(np.ones(X_val_.shape[0]), index=X_val_.index)\n",
    "        \n",
    "        # 去除的机构为oos\n",
    "        X_oos, y_oos = pd.concat([self.X_tr.loc[tr_idx, ], self.X_val.loc[val_idx, ]], axis=0) , pd.concat([self.y_tr.loc[tr_idx, ], self.y_val.loc[val_idx, ]], axis=0)\n",
    "        \n",
    "        # 为了避免oos与val相同则两者weight长度相同进而无法分辨使用哪个权重计算val的 ks auc, 对y_oos做去一操作\n",
    "        if len(y_val_) == len(y_oos):\n",
    "            X_oos, y_oos = X_oos.iloc[1:], y_oos.iloc[1:]\n",
    "        \n",
    "        weights = {'train':w_tr_, 'val': w_val_, 'oos':pd.Series(np.ones(len(y_oos)))}\n",
    "        \n",
    "        ## 默认不需要评估，当早停存在使用metric中的auc评估，当record_train_process==True时开启自定义评估\n",
    "        eval_results = {}\n",
    "        valid_sets, valid_names, feval, callbacks = None, None, None, []\n",
    "        train_set = lgb.Dataset(X_tr_, label=y_tr_, weight=w_tr_)\n",
    "        val_set = lgb.Dataset(X_val_, label=y_val_, reference=train_set)\n",
    "        oos_set = lgb.Dataset(X_oos, label=y_oos, reference=train_set)\n",
    "        \n",
    "        ## 判断是否需要早停，如果用户参数中给出了早停则固定最大迭代次数为300，否则不设置早停\n",
    "        if 'stopping_rounds' in param.keys():\n",
    "            param.update({'num_iterations': 300})\n",
    "            callbacks.append(lgb.early_stopping(stopping_rounds=param.get('stopping_rounds')))\n",
    "            ## 因为早停会每次迭代都评估 因为直接设为评估自定义函数\n",
    "            self.record_train_process = True\n",
    "            valid_sets = [train_set, val_set, oos_set]\n",
    "            valid_names = ['train', 'val', 'oos']\n",
    "        \n",
    "        ## 判断是否开启自定义评估\n",
    "        if self.record_train_process == True:\n",
    "            callbacks.append(lgb.record_evaluation(eval_results))\n",
    "            valid_sets = [train_set, val_set, oos_set]\n",
    "            valid_names = ['train', 'val', 'oos']\n",
    "            feval = self.weighted_metric(weights)\n",
    "        \n",
    "        # 训练模型, 评估训练 验证 oos\n",
    "        ## valid_set若非空则中第一个非训练集的数据集是唯一决定早停的依据，评价方式是param中的metric, 与feval无关\n",
    "        model = lgb.train(\n",
    "                          param,\n",
    "                          verbose_eval=0, \n",
    "                          train_set = train_set, \n",
    "                          valid_sets = valid_sets,\n",
    "                          #  feval = ['auc', get_ks_func, get_lift_func],\n",
    "                          #  eval_sample_weight = [w_tr_, w_val_, pd.series(np.ones(len(y_oos)))] 当切换lightgbm>=3.0.0可以使用\n",
    "                          feval = feval,\n",
    "                          valid_names = valid_names,\n",
    "                          callbacks = callbacks\n",
    "                         )\n",
    "        \n",
    "        if len(eval_results) > 0 and self.record_train_process == True:\n",
    "            eval_results = pd.DataFrame(eval_results)\n",
    "            eval_results['org'] = org\n",
    "        \n",
    "        if self.record_train_process == False:\n",
    "            auc_w, auc, ks_w, ks, lift5, lift10 = self.single_weighted_metric(model, X_tr_, y_tr_, w_tr_)\n",
    "            tmp0 = pd.DataFrame({'train':[[auc_w], [auc], [ks_w], [ks], [lift5], [lift10]]}, index=['auc_w', 'auc', 'ks_w', 'ks', '5%lift', '10%lift'])\n",
    "            auc_w, auc, ks_w, ks, lift5, lift10 = self.single_weighted_metric(model, X_val_, y_val_, w_val_)\n",
    "            tmp1 = pd.DataFrame({'val':[[auc_w], [auc], [ks_w], [ks], [lift5], [lift10]]}, index=['auc_w', 'auc', 'ks_w', 'ks', '5%lift', '10%lift'])\n",
    "            auc_w, auc, ks_w, ks, lift5, lift10 = self.single_weighted_metric(model, X_oos, y_oos, pd.Series(np.ones(len(y_oos))))\n",
    "            tmp2 = pd.DataFrame({'oos':[[auc_w], [auc], [ks_w], [ks], [lift5], [lift10]]}, index=['auc_w', 'auc', 'ks_w', 'ks', '5%lift', '10%lift'])\n",
    "            eval_results = pd.concat([tmp0, tmp1, tmp2], axis=1)\n",
    "            eval_results['org'] = org\n",
    "        \n",
    "        return eval_results\n",
    "\n",
    "    # 自定义目标函数，当参数符合要求时进一步更新超参数空间寻优\n",
    "    def objective(self, param):\n",
    "        \n",
    "        begin_time = time.time()\n",
    "        if self.fobj is not None:\n",
    "            param.update({'objective': self.fobj})\n",
    "        \n",
    "        results = pd.DataFrame()\n",
    "        \n",
    "        # 开启9个进程池运行lgb\n",
    "        tasks = [(org, param) for org in self.tr_orgidx.keys()]\n",
    "        with Pool(9) as pool:\n",
    "            records = pool.starmap(self.train_epoch_, tasks)\n",
    "        for record in records:\n",
    "            results = pd.concat([results, record], axis=0)\n",
    "        \n",
    "        simpler_results = self.extract_evalresult(results)\n",
    "        mean_val_ks, mean_oos_ks = np.mean(simpler_results['val_ks']), np.mean(simpler_results['oos_ks'])\n",
    "        \n",
    "        import json\n",
    "        \n",
    "        result = {'a':20}\n",
    "        with open(\"res.json\", 'w') as f:\n",
    "            json.dump(result, f)\n",
    "        \n",
    "        # 判断参数符合要求条件为每个机构做oos时的训练集和验证集ks差距在相对3%以下，否则不更新loss\n",
    "        if np.allclose(simpler_results['tr_auc'], simpler_results['val_auc'], rtol=self.auc_threshold) and np.allclose(simpler_results['tr_auc_w'], simpler_results['val_auc_w'], rtol=self.auc_threshold):\n",
    "            loss = -mean_oos_ks\n",
    "            status = STATUS_OK\n",
    "        else:\n",
    "            loss = np.Inf\n",
    "            status = STATUS_FAIL\n",
    "        \n",
    "        end_time = time.time()\n",
    "        display(f\"当前组参数训练耗时：{np.round((end_time-begin_time)*1.0/60, 2)}分\")\n",
    "        \n",
    "        return {'loss': loss, 'param':param, 'status':status, 'randn':self.randn,\n",
    "                'mean_val_ks':mean_val_ks, 'mean_oos_ks':mean_oos_ks, 'simpler_results':simpler_results, 'results': results}\n",
    "    \n",
    "    # 该类的执行函数，返回trails\n",
    "    def tpesearch_params(self):\n",
    "        begin_time_ = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "        display(f\"开始执行时间：{begin_time_}\")\n",
    "        _ = fmin(fn=self.objective, space=self.params, algo=tpe.suggest, max_evals=self.max_iterations, trials=self.trails)\n",
    "        return pd.DataFrame(self.trails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperopy参数寻优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_threads': 3, \n",
    "          'boosting_type': hp.choice('boosting_type', ['gbdt']),\n",
    "          'num_iterations': scope.int(hp.quniform('num_iterations',80, 300, 5)), \n",
    "          'learning_rate':hp.quniform('learning_rate', 0.02, 0.07, 0.01),\n",
    "          'colsample_bytree':hp.quniform('colsample_bytree',0.6, 0.95, 0.05),\n",
    "          'max_depth': scope.int(hp.quniform('max_depth',3, 6, 1)), \n",
    "          'max_bin': scope.int(hp.quniform('max_bin',80, 200, 20)), \n",
    "          'min_child_weight': scope.int(hp.quniform('min_child_weight', 10, 40, 5)),\n",
    "          'min_child_samples': scope.int(hp.quniform('min_child_samples', 20, 100, 20)),\n",
    "          'reg_alpha': hp.quniform('reg_alpha', 1, 10, 1),\n",
    "          'reg_lambda': hp.quniform('reg_lambda', 1, 10, 1),\n",
    "          'balanced_badrate': hp.quniform('balanced_badrate', 0.1, 0.5, 0.05),\n",
    "          'broadcast_with_tar': hp.choice('broadcast_with_tar', [False]), # 如果设置了balanced_badrate不为空 则参数不起作用\n",
    "          'objective':'binary',\n",
    "          'metric':'auc' # 请不要去掉此参数，如果有早停将作为依据\n",
    "}\n",
    "kwargs = {'data':data, 'params': params, 'fobj':None, 'max_iterations': 20, \"record_train_process\":False, 'auc_threshold':5e-2, 'randn':42}\n",
    "optlgb = HyperOptLGB(**kwargs)\n",
    "_ = optlgb.tpesearch_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trail记录形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.arange(len(optlgb.trails.results)):\n",
    "    if optlgb.trails.results[idx].get('loss')==-0.2845397894455245:\n",
    "        display(optlgb.trails.results[idx].get('param'))\n",
    "        #optlgb.trails.results[6].get('simpler_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最佳参数(单个)下重现训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "         'boosting_type': hp.choice('boosting_type', ['gbdt']),\n",
    "         'colsample_bytree': hp.choice('colsample_bytree', [0.8]),\n",
    "         'learning_rate': hp.choice('learning_rate', [0.05]),\n",
    "         'max_bin': hp.choice('max_bin', [120]),\n",
    "         'max_depth': hp.choice('max_depth', [5]),\n",
    "         'metric': 'auc',# 请不要去掉此参数，如果有早停将作为依据\n",
    "         'min_child_weight': hp.choice('min_child_weight', [35]),\n",
    "         'min_child_samples': hp.choice('min_child_samples', [60]),\n",
    "         'num_iterations': hp.choice('num_iterations', [275]),\n",
    "         'num_threads': 3,\n",
    "         'objective': 'binary',\n",
    "         'reg_alpha': hp.choice('reg_alpha', [3]),\n",
    "         'reg_lambda': hp.choice('reg_lambda', [2]),\n",
    "         'balanced_badrate': hp.choice('balanced_badrate', [0.30000000000000004]),\n",
    "         'broadcast_with_tar': hp.choice('broadcast_with_tar', [False])\n",
    "}\n",
    "\n",
    "kwargs = {'data':data, 'params': params, 'fobj':None, 'max_iterations': 1, \"record_train_process\":True, 'auc_threshold':5e-2, 'randn':42}\n",
    "optlgb1= HyperOptLGB(**kwargs)\n",
    "_ = optlgb1.tpesearch_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simper_results = optlgb1.trails.results[0].get('simpler_results')\n",
    "results = optlgb1.trails.results[0].get('results')\n",
    "results['idx'] = results.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simper_results.to_csv(\"tx process wo balanced/simper_results_new.csv\", index=False)\n",
    "results.to_csv(\"tx process wo balanced/results_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
