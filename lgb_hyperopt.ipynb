{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.utils.analysis import *\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"原始数据有741764条, 根据['mobile_org', 'apply_date', 'org']去重且只保留标签列[0,1]的数据\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'去重后数据有741764条'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'credit_target, org被重命名为new_target, new_org; apply_date被格式化为new_date, new_date_ym两列'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {'data_pth':'../../1/mj2非银版/data/modeling_data.csv',\n",
    "    'date_colName': 'apply_date',\n",
    "    'y_colName': 'credit_target',\n",
    "    'org_colName':'org',\n",
    "    'data_encode':'utf-8',\n",
    "    'key_colNames' : ['mobile_org', 'apply_date', 'org']}\n",
    "data = get_dataset(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_5_lift(pred_, data):\n",
    "    y = data.get_label()\n",
    "    pred = 1 / (1 + np.exp(-pred_))\n",
    "    lift5 = _get_lift(y, pred, 0.05)\n",
    "    return '5%lift', lift5, True\n",
    "def top_10_lift(pred_, data):\n",
    "    y = data.get_label()\n",
    "    pred = 1 / (1 + np.exp(-pred_))\n",
    "    lift10 = _get_lift(y, pred, 0.1)\n",
    "    return '10%lift', lift10, True \n",
    "def _get_lift(y, pred, k):\n",
    "    n_top = int(len(y) * k)\n",
    "    top_indices = pd.Series(pred).sort_values(ascending=False).head(n_top).index\n",
    "    return y[top_indices].mean() / y.mean()\n",
    "def _get_ks(model, X, y):\n",
    "    pred = model.predict(X)\n",
    "    ks = toad.metrics.KS(pred, y)\n",
    "    return ks\n",
    "\n",
    "class HyperOptLGB(object):\n",
    "    '''\n",
    "        init: data, params, fobj, max_iteration\n",
    "        funcs: \n",
    "            split_data()\n",
    "            train_epoch_()\n",
    "            objective()\n",
    "            tpesearch_params()\n",
    "        return Trails\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        self.data = kwargs.get('data')\n",
    "        self.params = kwargs.get('params')\n",
    "        self.fobj = kwargs.get('fobj')\n",
    "        self.max_iterations = kwargs.get('max_iterations')\n",
    "        self.X_tr, self.X_val, self.y_tr, self.y_val, self.tr_orgidx, self.val_orgidx = self.split_data(self.data)\n",
    "        # replace with weight-balance func, applied on entire dataset rather not trainset\n",
    "        self.w_tr = pd.Series(np.ones(self.X_tr.shape[0]))\n",
    "        self.trails = Trials()\n",
    "    \n",
    "    # 输入数据集，返回8:2切分的训练验证集 & 字典形式储存的各机构在训练集验证集的索引，确保输入的数据集有new_org, new_target列\n",
    "    def split_data(self, data_):\n",
    "        data = data_.copy()\n",
    "        # 确定X，判定条件不是object类型且不是Y列\n",
    "        feas = [v for v in data.columns if data[v].dtype!='O' and v!='new_target']\n",
    "        # tr_orgidx存储训练集各个机构索引，tr_idx存储训练集全部索引, val为验证集同样做法\n",
    "        tr_orgidx, val_orgidx, tr_idx, val_idx = {}, {}, [], []\n",
    "        # 分层抽样\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, random_state=42, train_size=0.8)\n",
    "        for org in data.new_org.unique():\n",
    "            tmp_data = data[data.new_org==org].copy()\n",
    "            # 每个机构下分层抽样\n",
    "            for idx_tr, idx_val in splitter.split(tmp_data[feas], tmp_data['new_target']):\n",
    "                tr_orgidx[org] = list(idx_tr)\n",
    "                val_orgidx[org] = list(idx_val)\n",
    "                val_idx += list(idx_val)\n",
    "                tr_idx += list(idx_tr)\n",
    "        # 分出训练、验证集\n",
    "        data_tr, data_val = data.loc[tr_idx, ], data.loc[val_idx, ]\n",
    "        X_tr, X_val, y_tr, y_val = data_tr[feas], data_val[feas], data_tr['new_target'], data_val['new_target']\n",
    "        return X_tr, X_val, y_tr, y_val, tr_orgidx, val_orgidx\n",
    "    \n",
    "    # 每组参数下分机构cv训练, 返回每个机构做oos下的train val oos ks, oos ks\n",
    "    def train_epoch_(self, org, param):\n",
    "        tr_idxs, val_idxs = set(self.X_tr.index), set(self.X_val.index)\n",
    "        tr_idx, val_idx = self.tr_orgidx.get(org), self.val_orgidx.get(org)\n",
    "        # 除去当前org选出训练验证集\n",
    "        X_tr_, y_tr_ = self.X_tr.loc[list(tr_idxs-set(tr_idx)), ], self.y_tr.loc[list(tr_idxs-set(tr_idx)), ]\n",
    "        # 获得训练集权重参数，weight后改为全部数据都有，根据训练集索引再取出需要的部分\n",
    "        w_tr_ = self.w_tr.loc[list(tr_idxs-set(tr_idx)), ]\n",
    "        X_val_, y_val_ = self.X_val.loc[list(val_idxs-set(val_idx)), ], self.y_val.loc[list(val_idxs-set(val_idx)), ]\n",
    "        # 去除的机构当为oos\n",
    "        X_oos, y_oos = pd.concat([self.X_tr.loc[tr_idx, ], self.X_val.loc[val_idx, ]], axis=0) , pd.concat([self.y_tr.loc[tr_idx, ], self.y_val.loc[val_idx, ]], axis=0)\n",
    "        # 自动判断是否需要早停，如果用户参数中给出了早停则固定最大迭代次数为300，否则不设置早停\n",
    "        callbacks = None\n",
    "        if 'stopping_rounds' in param.keys():\n",
    "            param.update({'num_iterations': 300})\n",
    "            callbacks = [lgb.early_stopping(stopping_rounds=param.get('stopping_rounds'))]\n",
    "        # 训练模型, 仅评估验证集\n",
    "        model = lgb.train(\n",
    "                          param,\n",
    "                          verbose_eval=0, \n",
    "                          train_set = lgb.Dataset(X_tr_, label=y_tr_, weight=w_tr_), \n",
    "                          valid_sets = [lgb.Dataset(X_val_, label=y_val_)],\n",
    "                          valid_names = ['train', 'val'],\n",
    "                          callbacks = callbacks\n",
    "                         )\n",
    "        ks_tr, ks_val, ks_oos = _get_ks(model, X_tr_, y_tr_), _get_ks(model, X_val_, y_val_), _get_ks(model, X_oos, y_oos)\n",
    "        record = pd.DataFrame({'oos_org':org, 'train_ks':ks_tr, 'val_ks':ks_val, 'oos_ks':ks_oos}, index=['0'])\n",
    "        return record\n",
    "\n",
    "    # 自定义目标函数，当参数符合要求时进一步更新超参数空间寻优\n",
    "    def objective(self, param):\n",
    "        begin_time = time.time()\n",
    "        if self.fobj is not None:\n",
    "            param.update({'objective': self.fobj})\n",
    "        results = pd.DataFrame()\n",
    "        # 开启9个进程池运行lgb\n",
    "        tasks = [(org, param) for org in self.tr_orgidx.keys()]\n",
    "        with Pool(9) as pool:\n",
    "            records = pool.starmap(self.train_epoch_, tasks)\n",
    "        for record in records:\n",
    "            results = pd.concat([results, record], axis=0)\n",
    "        \n",
    "        mean_tr_ks = np.mean(results['train_ks'])\n",
    "        mean_val_ks = np.mean(results['val_ks'])\n",
    "        mean_oos_ks = (np.sum(results['oos_ks'])-np.min(results['oos_ks'])-np.max(results['oos_ks']))*1.0 / (results.shape[0]-2)\n",
    "        # 判断参数符合要求条件为每个机构做oos时的训练集和验证集ks差距在相对3%以下，否则不更新loss\n",
    "        if np.allclose(results['train_ks'], results['val_ks'], rtol=3e-2):\n",
    "            loss = -(0.5*mean_val_ks + 0.5*mean_oos_ks)\n",
    "            status = STATUS_OK\n",
    "        else:\n",
    "            loss = np.Inf\n",
    "            status = STATUS_FAIL\n",
    "        end_time = time.time()\n",
    "        display(f\"当前组参数训练耗时：{np.round((end_time-begin_time)*1.0/60, 2)}分\")\n",
    "        return {'loss': loss, 'param':param, 'status':status}\n",
    "    \n",
    "    # 该类的执行函数，返回trails\n",
    "    def tpesearch_params(self):\n",
    "        begin_time_ = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "        display(f\"开始执行时间：{begin_time_}\")\n",
    "        _ = fmin(fn=self.objective, space=self.params, algo=tpe.suggest, max_evals=self.max_iterations, trials=self.trails)\n",
    "        return pd.DataFrame(self.trails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'开始执行时间：2025-06-11 17:44:33'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'当前组参数训练耗时：1.5分'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [01:30<1:13:36, 90.14s/it, best loss: ?]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'当前组参数训练耗时：1.52分'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [03:01<1:12:20, 90.42s/it, best loss: ?]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'当前组参数训练耗时：1.58分'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [04:36<1:11:51, 91.74s/it, best loss: ?]"
     ]
    }
   ],
   "source": [
    "params = {'num_threads': 3, \n",
    "          'num_iterations': scope.int(hp.quniform('num_iterations',70, 150, 5)), \n",
    "          'learning_rate':hp.quniform('learning_rate', 0.01, 0.05, 0.01),\n",
    "          'colsample_bytree':hp.quniform('colsample_bytree',0.5, 0.9, 0.1),\n",
    "          'max_depth': scope.int(hp.quniform('max_depth',3, 7, 1)), \n",
    "          'max_bin': scope.int(hp.quniform('max_bin',50, 150, 10)), \n",
    "          'min_child_weight': scope.int(hp.quniform('min_child_weight', 10, 30, 5)),\n",
    "          'reg_alpha': hp.quniform('reg_alpha', 1, 10, 1),\n",
    "          'reg_lambda': hp.quniform('reg_lambda', 1, 10, 1),\n",
    "          'objective':'binary', \n",
    "          'metric':'auc'\n",
    "}\n",
    "kwargs = {'data':data, 'params': params, 'fobj':None, 'max_iterations': 50}\n",
    "optlgb = HyperOptLGB(**kwargs)\n",
    "trails = optlgb.tpesearch_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
