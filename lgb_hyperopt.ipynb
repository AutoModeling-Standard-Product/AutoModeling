{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n"
     ]
    }
   ],
   "source": [
    "from main.utils.analysis import *\n",
    "from main.utils.data_augmentation import *\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 假设这里输入的最后用于建模数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据有741764条, 根据['mobile_org', 'apply_date', 'org']去重且只保留标签列[0,1]的数据\n",
      "去重后数据有741764条\n",
      "credit_target, org被重命名为new_target, new_org; apply_date被格式化为new_date, new_date_ym两列\n"
     ]
    }
   ],
   "source": [
    "params = {'data_pth':'../../1/mj2非银版/data/modeling_data.csv',\n",
    "    'date_colName': 'apply_date',\n",
    "    'y_colName': 'credit_target',\n",
    "    'org_colName':'org',\n",
    "    'data_encode':'utf-8',\n",
    "    'key_colNames' : ['mobile_org', 'apply_date', 'org']}\n",
    "data = get_dataset(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperOptLGB(object):\n",
    "    '''\n",
    "        init: data, params, fobj, weights, max_iteration\n",
    "        \n",
    "        excuting func: tpesearch_params()\n",
    "        \n",
    "        return: train log, trails\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        self.data = kwargs.get('data')\n",
    "        self.params = kwargs.get('params')\n",
    "        self.fobj = kwargs.get('fobj')\n",
    "        self.max_iterations = kwargs.get('max_iterations')\n",
    "        self.record_train_process = kwargs.get(\"record_train_process\")\n",
    "        self.auc_threshold = kwargs.get('auc_threshold')\n",
    "        self.randn = kwargs.get('randn')\n",
    "        self.X_tr, self.X_val, self.y_tr, self.y_val, self.tr_orgidx, self.val_orgidx = self.split_data(self.data)\n",
    "        self.trails = Trials()\n",
    "    \n",
    "    # 输入数据集，返回8:2切分的训练验证集 & 字典形式储存的各机构在训练集验证集的索引，确保输入的数据集有new_org, new_target列\n",
    "    def split_data(self, data):\n",
    "        \n",
    "        # 确定X，判定条件不是object类型且不是Y列\n",
    "        feas = [v for v in data.columns if data[v].dtype!='O' and v!='new_target']\n",
    "        \n",
    "        # tr_orgidx存储训练集各个机构索引，tr_idx存储训练集全部索引, val为验证集同样做法\n",
    "        tr_orgidx, val_orgidx, tr_idx, val_idx = {}, {}, [], []\n",
    "        \n",
    "        # 分层抽样\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, random_state=self.randn, train_size=0.8)\n",
    "        \n",
    "        for org in data.new_org.unique():\n",
    "            tmp_data = data[data.new_org==org].copy()\n",
    "            org_index = tmp_data.index\n",
    "            \n",
    "            # 每个机构下分层抽样, 注意不要使用相对索引否则会造成取值错误, 这里使用了绝对索引\n",
    "            for idx_tr, idx_val in splitter.split(tmp_data[feas], tmp_data['new_target']):\n",
    "                tr_orgidx[org] = list(org_index[idx_tr])\n",
    "                val_orgidx[org] = list(org_index[idx_val])\n",
    "                val_idx += list(org_index[idx_val])\n",
    "                tr_idx += list(org_index[idx_tr])\n",
    "        \n",
    "        # 分出训练、验证集\n",
    "        data_tr, data_val = data.loc[tr_idx, ], data.loc[val_idx, ]\n",
    "        X_tr, X_val, y_tr, y_val = data_tr[feas], data_val[feas], data_tr['new_target'], data_val['new_target']\n",
    "        return X_tr, X_val, y_tr, y_val, tr_orgidx, val_orgidx\n",
    "    \n",
    "    def _get_lift(self, y, pred, k):\n",
    "        \n",
    "        n_top = int(len(y) * k)\n",
    "        top_indices = pd.Series(pred).sort_values(ascending=False).head(n_top).index\n",
    "        \n",
    "        return y[top_indices].mean() / y.mean()\n",
    "            \n",
    "    ## 自定义feval评价函数，闭包方式，输入加权&不加权ks auc等, lightgbm==2.3.0时每次迭代都会强制评估无法跳过, 每次评估约为0.2-0.8秒均值为0.3秒\n",
    "    def weighted_metric(self, weights):\n",
    "        \n",
    "        def _weighted_metric(pred_, data):\n",
    "            y = data.get_label()\n",
    "            pred = 1 / (1 + np.exp(-pred_))\n",
    "            \n",
    "            ## 比较输入的数据长度和w_val_, w_tr_, w_oos_判断使用哪个权重\n",
    "            if len(y) == len(weights.get('train')):\n",
    "                w = weights.get('train')\n",
    "            elif len(y) == len(weights.get('val')):\n",
    "                w = weights.get('val')\n",
    "            else:\n",
    "                w = weights.get('oos')\n",
    "                \n",
    "            auc_w= roc_auc_score(y, pred, sample_weight=w)\n",
    "            fpr, tpr, _ = roc_curve(y, pred, sample_weight=w)\n",
    "            \n",
    "            ks = toad.metrics.KS(pred, y)\n",
    "            ks_w = max(tpr - fpr)\n",
    "            \n",
    "            lift10, lift5 = self._get_lift(y, pred, 0.1), self._get_lift(y, pred, 0.05)\n",
    "            \n",
    "            return [\n",
    "                    ('auc_w',auc_w,True),\n",
    "                    ('ks_w',ks_w,True), ('ks',ks,True),\n",
    "                    ('5%lift',lift5,True), ('10%lift',lift10,True)\n",
    "                ]\n",
    "        \n",
    "        return _weighted_metric\n",
    "    \n",
    "    ## 根据model和输入数据返回带权重&不带权重计算下的auc ks，替代weighted_metric用，只计算最后一次\n",
    "    def single_weighted_metric(self, model, X, y, w):\n",
    "        pred = model.predict(X)\n",
    "        pred = pd.Series(pred, index=X.index)\n",
    "        auc_w, auc = roc_auc_score(y, pred, sample_weight=w), roc_auc_score(y, pred)\n",
    "        fpr, tpr, _ = roc_curve(y, pred, sample_weight=w)\n",
    "        \n",
    "        ks = toad.metrics.KS(pred, y)\n",
    "        ks_w = max(tpr - fpr)\n",
    "        \n",
    "        lift10, lift5 = self._get_lift(y, pred, 0.1), self._get_lift(y, pred, 0.05)\n",
    "        \n",
    "        return auc_w, auc, ks_w, ks, lift5, lift10\n",
    "    \n",
    "    ## 提取训练日志中的最后一次迭代结果\n",
    "    def extract_evalresult(self, records):\n",
    "        \n",
    "        simpler_records = []\n",
    "        \n",
    "        for org in records.org.unique():\n",
    "            tmp_record = records[records.org==org].copy()\n",
    "            \n",
    "            simpler_records.append({\n",
    "                'org': org, 'tr_auc':tmp_record.train.auc[-1], 'tr_auc_w':tmp_record.train.auc_w[-1],\n",
    "                'tr_ks':tmp_record.train.ks[-1], 'tr_ks_w': tmp_record.train.ks_w[-1],\n",
    "                'tr_5lift':tmp_record.train['5%lift'][-1], 'tr_10lift':tmp_record.train['10%lift'][-1],\n",
    "                'val_auc':tmp_record.val.auc[-1], 'val_auc_w':tmp_record.val.auc_w[-1],\n",
    "                'val_ks':tmp_record.val.ks[-1], 'val_ks_w':tmp_record.val.ks_w[-1],\n",
    "                'val_5lift':tmp_record.val['5%lift'][-1], 'val_10lift':tmp_record.val['10%lift'][-1],\n",
    "                'oos_auc': tmp_record.oos.auc[-1], 'oos_ks':tmp_record.oos.ks[-1],\n",
    "                'oos_5lift':tmp_record.oos['5%lift'][-1], 'oos_10lift':tmp_record.oos['10%lift'][-1]\n",
    "                })\n",
    "        \n",
    "        simpler_records = pd.DataFrame(simpler_records)\n",
    "        \n",
    "        return simpler_records\n",
    "    \n",
    "    # 每组参数下分机构cv训练, 返回每个机构做oos下的train val oos ks, oos ks\n",
    "    def train_epoch_(self, org, param):\n",
    "        \n",
    "        broadcast_with_tar = param.get('broadcast_with_tar')\n",
    "        balanced_badrate = param.get(\"balanced_badrate\")\n",
    "        \n",
    "        ## 根据超参数坏样率得出权重weight\n",
    "        weight_tr = re_weight_by_org(self.y_tr, self.tr_orgidx, 0.5, broadcast_with_tar, balanced_badrate)\n",
    "        weight_val = re_weight_by_org(self.y_val, self.val_orgidx, 0.5, broadcast_with_tar, balanced_badrate)\n",
    "        weight = pd.concat([weight_tr, weight_val], axis=0)\n",
    "        #weight = pd.Series(np.where(self.data['new_target']==1, 4, 1), index=self.data.index)\n",
    "        \n",
    "        tr_idxs, val_idxs = set(self.X_tr.index), set(self.X_val.index)\n",
    "        tr_idx, val_idx = self.tr_orgidx.get(org), self.val_orgidx.get(org)\n",
    "        \n",
    "        # 除去当前org选出训练验证集\n",
    "        X_tr_, y_tr_ = self.X_tr.loc[list(tr_idxs-set(tr_idx)), ], self.y_tr.loc[list(tr_idxs-set(tr_idx)), ]\n",
    "        X_val_, y_val_ = self.X_val.loc[list(val_idxs-set(val_idx)), ], self.y_val.loc[list(val_idxs-set(val_idx)), ]\n",
    "        w_tr_, w_val_ = weight.loc[list(tr_idxs-set(tr_idx)), ], weight.loc[list(val_idxs-set(val_idx)), ]\n",
    "        #w_tr_ = pd.Series(np.ones(X_tr_.shape[0]))\n",
    "        \n",
    "        # 去除的机构为oos\n",
    "        X_oos, y_oos = pd.concat([self.X_tr.loc[tr_idx, ], self.X_val.loc[val_idx, ]], axis=0) , pd.concat([self.y_tr.loc[tr_idx, ], self.y_val.loc[val_idx, ]], axis=0)\n",
    "        \n",
    "        # 为了避免oos与val相同则两者weight长度相同进而无法分辨使用哪个权重计算val的 ks auc, 对y_oos做去一操作\n",
    "        if len(y_val_) == len(y_oos):\n",
    "            X_oos, y_oos = X_oos.iloc[1:], y_oos.iloc[1:]\n",
    "        \n",
    "        weights = {'train':w_tr_, 'val': w_val_, 'oos':pd.Series(np.ones(len(y_oos)))}\n",
    "        \n",
    "        ## 默认不需要评估，当早停存在使用metric中的auc评估，当record_train_process==True时开启自定义评估\n",
    "        eval_results = {}\n",
    "        valid_sets, valid_names, feval, callbacks = None, None, None, []\n",
    "        train_set = lgb.Dataset(X_tr_, label=y_tr_, weight=w_tr_)\n",
    "        val_set = lgb.Dataset(X_val_, label=y_val_, reference=train_set)\n",
    "        oos_set = lgb.Dataset(X_oos, label=y_oos, reference=train_set)\n",
    "        \n",
    "        ## 判断是否需要早停，如果用户参数中给出了早停则固定最大迭代次数为300，否则不设置早停\n",
    "        if 'stopping_rounds' in param.keys():\n",
    "            param.update({'num_iterations': 300})\n",
    "            callbacks.append(lgb.early_stopping(stopping_rounds=param.get('stopping_rounds')))\n",
    "            ## 因为早停会每次迭代都评估 因为直接设为评估自定义函数\n",
    "            self.record_train_process = True\n",
    "            valid_sets = [train_set, val_set, oos_set]\n",
    "            valid_names = ['train', 'val', 'oos']\n",
    "        \n",
    "        ## 判断是否开启自定义评估\n",
    "        if self.record_train_process == True:\n",
    "            callbacks.append(lgb.record_evaluation(eval_results))\n",
    "            valid_sets = [train_set, val_set, oos_set]\n",
    "            valid_names = ['train', 'val', 'oos']\n",
    "            feval = self.weighted_metric(weights)\n",
    "        \n",
    "        # 训练模型, 评估训练 验证 oos\n",
    "        ## valid_set若非空则中第一个非训练集的数据集是唯一决定早停的依据，评价方式是param中的metric, 与feval无关\n",
    "        model = lgb.train(\n",
    "                          param,\n",
    "                          verbose_eval=0, \n",
    "                          train_set = train_set, \n",
    "                          valid_sets = valid_sets,\n",
    "                          #  feval = ['auc', get_ks_func, get_lift_func],\n",
    "                          #  eval_sample_weight = [w_tr_, w_val_, pd.series(np.ones(len(y_oos)))] 当切换lightgbm>=3.0.0可以使用\n",
    "                          feval = feval,\n",
    "                          valid_names = valid_names,\n",
    "                          callbacks = callbacks\n",
    "                         )\n",
    "        \n",
    "        if len(eval_results) > 0 and self.record_train_process == True:\n",
    "            eval_results = pd.DataFrame(eval_results)\n",
    "            eval_results['org'] = org\n",
    "        \n",
    "        if self.record_train_process == False:\n",
    "            auc_w, auc, ks_w, ks, lift5, lift10 = self.single_weighted_metric(model, X_tr_, y_tr_, w_tr_)\n",
    "            tmp0 = pd.DataFrame({'train':[[auc_w], [auc], [ks_w], [ks], [lift5], [lift10]]}, index=['auc_w', 'auc', 'ks_w', 'ks', '5%lift', '10%lift'])\n",
    "            auc_w, auc, ks_w, ks, lift5, lift10 = self.single_weighted_metric(model, X_val_, y_val_, w_val_)\n",
    "            tmp1 = pd.DataFrame({'val':[[auc_w], [auc], [ks_w], [ks], [lift5], [lift10]]}, index=['auc_w', 'auc', 'ks_w', 'ks', '5%lift', '10%lift'])\n",
    "            auc_w, auc, ks_w, ks, lift5, lift10 = self.single_weighted_metric(model, X_oos, y_oos, pd.Series(np.ones(len(y_oos))))\n",
    "            tmp2 = pd.DataFrame({'oos':[[auc_w], [auc], [ks_w], [ks], [lift5], [lift10]]}, index=['auc_w', 'auc', 'ks_w', 'ks', '5%lift', '10%lift'])\n",
    "            eval_results = pd.concat([tmp0, tmp1, tmp2], axis=1)\n",
    "            eval_results['org'] = org\n",
    "        \n",
    "        return eval_results\n",
    "\n",
    "    # 自定义目标函数，当参数符合要求时进一步更新超参数空间寻优\n",
    "    def objective(self, param):\n",
    "        \n",
    "        begin_time = time.time()\n",
    "        if self.fobj is not None:\n",
    "            param.update({'objective': self.fobj})\n",
    "        \n",
    "        results = pd.DataFrame()\n",
    "        \n",
    "        # 开启9个进程池运行lgb\n",
    "        tasks = [(org, param) for org in self.tr_orgidx.keys()]\n",
    "        with Pool(5) as pool:\n",
    "            records = pool.starmap(self.train_epoch_, tasks)\n",
    "        for record in records:\n",
    "            results = pd.concat([results, record], axis=0)\n",
    "        \n",
    "        simpler_results = self.extract_evalresult(results)\n",
    "        mean_val_ks, mean_oos_ks = np.mean(simpler_results['val_ks']), np.mean(simpler_results['oos_ks'])\n",
    "        \n",
    "        # 判断参数符合要求条件为每个机构做oos时的训练集和验证集ks差距在相对3%以下，否则不更新loss\n",
    "        if np.allclose(simpler_results['tr_auc'], simpler_results['val_auc'], rtol=self.auc_threshold) and np.allclose(simpler_results['tr_auc_w'], simpler_results['val_auc_w'], rtol=self.auc_threshold):\n",
    "            loss = -(0.5*mean_val_ks + 0.5*mean_oos_ks)\n",
    "            status = STATUS_OK\n",
    "        else:\n",
    "            loss = np.Inf\n",
    "            status = STATUS_FAIL\n",
    "        \n",
    "        end_time = time.time()\n",
    "        display(f\"当前组参数训练耗时：{np.round((end_time-begin_time)*1.0/60, 2)}分\")\n",
    "        \n",
    "        return {'loss': loss, 'param':param, 'status':status, 'randn':self.randn,\n",
    "                'mean_val_ks':mean_val_ks, 'mean_oos_ks':mean_oos_ks, 'simpler_results':simpler_results, 'results': results}\n",
    "    \n",
    "    # 该类的执行函数，返回trails\n",
    "    def tpesearch_params(self):\n",
    "        begin_time_ = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "        display(f\"开始执行时间：{begin_time_}\")\n",
    "        _ = fmin(fn=self.objective, space=self.params, algo=tpe.suggest, max_evals=self.max_iterations, trials=self.trails)\n",
    "        return pd.DataFrame(self.trails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperopy参数寻优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'开始执行时间：2025-06-30 09:25:33'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'当前组参数训练耗时：1.05分'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [01:02<04:11, 62.93s/it, best loss: ?]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'当前组参数训练耗时：1.06分'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [02:06<03:09, 63.21s/it, best loss: ?]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'当前组参数训练耗时：1.15分'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [03:15<02:09, 64.97s/it, best loss: -0.19268886296776758]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'当前组参数训练耗时：1.16分'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [04:25<01:06, 66.37s/it, best loss: -0.19268886296776758]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'当前组参数训练耗时：1.08分'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:30<00:00, 65.82s/it, best loss: -0.19268886296776758]\n"
     ]
    }
   ],
   "source": [
    "params = {'num_threads': 3, \n",
    "          'num_iterations': scope.int(hp.quniform('num_iterations',80, 120, 5)), \n",
    "          'learning_rate':hp.quniform('learning_rate', 0.01, 0.05, 0.01),\n",
    "          'colsample_bytree':hp.quniform('colsample_bytree',0.5, 0.9, 0.1),\n",
    "          'max_depth': scope.int(hp.quniform('max_depth',3, 7, 1)), \n",
    "          'max_bin': scope.int(hp.quniform('max_bin',50, 150, 10)), \n",
    "          'min_child_weight': scope.int(hp.quniform('min_child_weight', 10, 30, 5)),\n",
    "          'reg_alpha': hp.quniform('reg_alpha', 1, 10, 1),\n",
    "          'reg_lambda': hp.quniform('reg_lambda', 1, 10, 1),\n",
    "          'balanced_badrate': hp.quniform('balanced_badrate', 0.1, 0.3, 0.05),\n",
    "          'broadcast_with_tar': hp.choice('broadcast_with_tar', [False]), # 如果设置了balanced_badrate不为空 则参数不起作用\n",
    "          'objective':'binary',\n",
    "          'metric':'auc' # 请不要去掉此参数，如果有早停将作为依据\n",
    "}\n",
    "kwargs = {'data':data, 'params': params, 'fobj':None, 'max_iterations': 5, \"record_train_process\":False, 'auc_threshold':8e-2, 'randn':42}\n",
    "optlgb = HyperOptLGB(**kwargs)\n",
    "_ = optlgb.tpesearch_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trail记录形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'balanced_badrate': 0.30000000000000004,\n",
       " 'broadcast_with_tar': False,\n",
       " 'colsample_bytree': 0.8,\n",
       " 'learning_rate': 0.03,\n",
       " 'max_bin': 100,\n",
       " 'max_depth': 4,\n",
       " 'metric': 'auc',\n",
       " 'min_child_weight': 15,\n",
       " 'num_iterations': 120,\n",
       " 'num_threads': 3,\n",
       " 'objective': 'binary',\n",
       " 'reg_alpha': 8.0,\n",
       " 'reg_lambda': 4.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(optlgb.trails.results[2].get('param'))\n",
    "        #optlgb.trails.results[6].get('simpler_results'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最佳参数(单个)下重现训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'开始执行时间：2025-06-30 09:36:08'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'当前组参数训练耗时：5.34分'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [05:20<00:00, 320.51s/it, best loss: -0.19265513535283693]\n"
     ]
    }
   ],
   "source": [
    "params = {'colsample_bytree': hp.choice('colsample_bytree', [0.8]),\n",
    "         'learning_rate': hp.choice('learning_rate', [0.03]),\n",
    "         'max_bin': hp.choice('max_bin', [100]),\n",
    "         'max_depth': hp.choice('max_depth', [4]),\n",
    "         'metric': 'auc',# 请不要去掉此参数，如果有早停将作为依据\n",
    "         'min_child_weight': hp.choice('min_child_weight', [15]),\n",
    "         'num_iterations': hp.choice('num_iterations', [120]),\n",
    "         'num_threads': 3,\n",
    "         'objective': 'binary',\n",
    "         'reg_alpha': hp.choice('reg_alpha', [8.0]),\n",
    "         'reg_lambda': hp.choice('reg_lambda', [4.0]),\n",
    "         'balanced_badrate': hp.choice('balanced_badrate', [0.30000000000000004]),\n",
    "         'broadcast_with_tar': hp.choice('broadcast_with_tar', [False])\n",
    "}\n",
    "\n",
    "kwargs = {'data':data, 'params': params, 'fobj':None, 'max_iterations': 1, \"record_train_process\":True, 'auc_threshold':8e-2}\n",
    "optlgb1= HyperOptLGB(**kwargs)\n",
    "_ = optlgb1.tpesearch_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "simper_results = optlgb1.trails.results[0].get('simpler_results')\n",
    "results = optlgb1.trails.results[0].get('results')\n",
    "results['idx'] = results.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "simper_results.to_csv(\"process results/simper_results.csv\", index=False)\n",
    "results.to_csv(\"process results/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "      <th>oos</th>\n",
       "      <th>org</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>auc</td>\n",
       "      <td>[0.6184021628567696, 0.6234755514792742, 0.628...</td>\n",
       "      <td>[0.6363860817146368, 0.6417266708550553, 0.647...</td>\n",
       "      <td>[0.5566236427526062, 0.5578047609977781, 0.561...</td>\n",
       "      <td>xk_hc</td>\n",
       "      <td>auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>auc_w</td>\n",
       "      <td>[0.6184021634850708, 0.6234755519719553, 0.628...</td>\n",
       "      <td>[0.600367694332157, 0.6071559150009227, 0.6167...</td>\n",
       "      <td>[0.5566236427526063, 0.5578047609977781, 0.561...</td>\n",
       "      <td>xk_hc</td>\n",
       "      <td>auc_w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   train  \\\n",
       "auc    [0.6184021628567696, 0.6234755514792742, 0.628...   \n",
       "auc_w  [0.6184021634850708, 0.6234755519719553, 0.628...   \n",
       "\n",
       "                                                     val  \\\n",
       "auc    [0.6363860817146368, 0.6417266708550553, 0.647...   \n",
       "auc_w  [0.600367694332157, 0.6071559150009227, 0.6167...   \n",
       "\n",
       "                                                     oos    org    idx  \n",
       "auc    [0.5566236427526062, 0.5578047609977781, 0.561...  xk_hc    auc  \n",
       "auc_w  [0.5566236427526063, 0.5578047609977781, 0.561...  xk_hc  auc_w  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
