{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.utils.analysis import *\n",
    "from main.utils.data_augmentation import *\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# 内存使用\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"总内存: {mem.total / (1024**3):.1f} GB\")\n",
    "print(f\"已用内存: {mem.used / (1024**3):.1f} GB\")\n",
    "\n",
    "# CPU使用\n",
    "print(f\"CPU核心数: {psutil.cpu_count(logical=True)}\")\n",
    "print(f\"当前CPU占用: {psutil.cpu_percent()}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_feas = pd.read_csv(\"data/feas_x/tx_fined_feas.csv\")\n",
    "supply_feas = pd.read_csv(\"data/feas_x/tx_supplyment_feas.csv\")\n",
    "feas = set(list(fine_feas['0']) + list(supply_feas['0']) + ['matchingid', 'org', 'target', 'apply_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'data_pth':'data/tx_unfilterdata.csv',\n",
    "    'date_colName': 'apply_date',\n",
    "    'y_colName': 'target',\n",
    "    'org_colName':'org',\n",
    "    'data_encode':'utf-8',\n",
    "    'key_colNames' : ['matchingid'],\n",
    "    'use_cols':feas\n",
    "}\n",
    "data = get_dataset(**params)\n",
    "\n",
    "params = {\n",
    "    'data': data,\n",
    "    'minYmBadsample': 5,\n",
    "    'minYmSample': 0\n",
    "}\n",
    "data = drop_abnormal_ym(**params)\n",
    "\n",
    "data.replace({-1111:np.nan, -999:np.nan, -1:np.nan}, inplace=True)\n",
    "\n",
    "oos_data = data[data.new_org.isin(['202412050001_光大信用卡_分期', '202408260001_上海银行', '202412040001_中原银行',\n",
    "                  '202412040002_中原银行', '202406140001_分期乐_欺诈', '202403280002_长银消金_唯品会',\n",
    "                 '202504210002_华通_360', '202408130001_洋钱罐', '202411040001_汇登数字', '202412090001_广州农商行'])]\n",
    "\n",
    "data = data[~data.new_org.isin(['202412050001_光大信用卡_分期', '202408260001_上海银行', '202412040001_中原银行',\n",
    "                  '202412040002_中原银行', '202406140001_分期乐_欺诈', '202403280002_长银消金_唯品会',\n",
    "                 '202504210002_华通_360', '202408130001_洋钱罐', '202411040001_汇登数字', '202412090001_广州农商行'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference(object):\n",
    "    '''\n",
    "        init: data, oos_data, param, results, child_score, score_name, store_pth, randn, score_transform_func, model(optional)\n",
    "        funcs: \n",
    "        \n",
    "        return model, train log figures, model score figures, report(xlsx)\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        self.data = kwargs.get('data')\n",
    "        self.oos_data = kwargs.get('oos_data')\n",
    "        self.param = kwargs.get('param')\n",
    "        self.param['num_leaves'] = 2**self.param.get('max_depth')-1\n",
    "        self.results = kwargs.get(\"results\")\n",
    "        self.dataset_statis = kwargs.get(\"dataset_statis\")\n",
    "        self.child_score = kwargs.get(\"child_score\")\n",
    "        self.randn = kwargs.get('randn')\n",
    "        self.score_name = kwargs.get('score_name')\n",
    "        self.store_pth = kwargs.get(\"store_pth\")\n",
    "        self.score_transform_func = kwargs.get(\"score_transform_func\")\n",
    "        self.model = kwargs.get(\"model\")\n",
    "        self.feas_gain = None\n",
    "        self.X_tr = None\n",
    "        self.y_tr = None\n",
    "        self.w_tr = None\n",
    "        self.tr_orgidx = None\n",
    "    \n",
    "    def inference_split_data(self, data, flag):\n",
    "        \n",
    "        # 确定X，判定条件不是object类型且不是Y列\n",
    "        feas = [v for v in data.columns if data[v].dtype!='O' and v!='new_target']\n",
    "        \n",
    "        # tr_orgidx存储训练集各个机构索引，tr_idx存储训练集全部索引, val为验证集同样做法\n",
    "        tr_orgidx, val_orgidx, tr_idx, val_idx = {}, {}, [], []\n",
    "        \n",
    "        if flag==True:\n",
    "            # 分层抽样\n",
    "            splitter = StratifiedShuffleSplit(n_splits=1, random_state=self.randn, train_size=0.8)\n",
    "\n",
    "            for org in data.new_org.unique():\n",
    "                tmp_data = data[data.new_org==org].copy()\n",
    "                org_index = tmp_data.index\n",
    "\n",
    "                # 每个机构下分层抽样, 注意不要使用相对索引否则会造成取值错误, 这里使用了绝对索引\n",
    "                for idx_tr, idx_val in splitter.split(tmp_data[feas], tmp_data['new_target']):\n",
    "                    tr_orgidx[org] = list(org_index[idx_tr])\n",
    "                    val_orgidx[org] = list(org_index[idx_val])\n",
    "                    val_idx += list(org_index[idx_val])\n",
    "                    tr_idx += list(org_index[idx_tr])\n",
    "            \n",
    "            data_tr, data_val = data.loc[tr_idx, ], data.loc[val_idx, ]\n",
    "            X_tr, X_val, y_tr, y_val = data_tr[feas], data_val[feas], data_tr['new_target'], data_val['new_target']\n",
    "            return X_tr, X_val, y_tr, y_val, tr_orgidx, val_orgidx\n",
    "           \n",
    "        else:\n",
    "            for org in data.new_org.unique():\n",
    "                tmp_data = data[data.new_org==org].copy()\n",
    "                org_index = tmp_data.index\n",
    "                tr_orgidx[org] = list(org_index)\n",
    "                tr_idx += list(org_index)\n",
    "        \n",
    "            data_tr = data.loc[tr_idx, ]\n",
    "            X_tr, y_tr = data_tr[feas], data_tr['new_target']\n",
    "            return X_tr, None, y_tr, None, tr_orgidx, None\n",
    "    \n",
    "    def _get_lift(self, y, pred, k):\n",
    "        \n",
    "        n_top = int(len(y) * k)\n",
    "        top_indices = pd.Series(pred, index=y.index).sort_values(ascending=False).head(n_top).index\n",
    "        \n",
    "        return y[top_indices].mean() / y.mean()\n",
    "    \n",
    "    ## 为查看在n-1个机构上全量样本上训练的模型效果设置\n",
    "    def inference_oos_metric(self, model, X, y):\n",
    "        pred = model.predict(X)\n",
    "        pred = pd.Series(pred, index=X.index)\n",
    "        \n",
    "        auc = roc_auc_score(y, pred)\n",
    "        ks = toad.metrics.KS(pred, y)\n",
    "        \n",
    "        lift10, lift5, lift3 = self._get_lift(y, pred, 0.1), self._get_lift(y, pred, 0.05), self._get_lift(y, pred, 0.03)\n",
    "        \n",
    "        return  np.round(auc, 3), np.round(ks, 3), np.round(lift3, 2), np.round(lift5, 2), np.round(lift10, 2)\n",
    "    \n",
    "    ## 为子分生成评估metric\n",
    "    def inference_childscore_metric(self, x, y):\n",
    "        auc = roc_auc_score(y, x)\n",
    "        ks = toad.metrics.KS(x, y)\n",
    "        \n",
    "        lift10, lift5, lift3 = self._get_lift(y, x, 0.1), self._get_lift(y, x, 0.05), self._get_lift(y, x, 0.03)\n",
    "        \n",
    "        return np.round(auc, 3), np.round(ks, 3), np.round(lift3, 2), np.round(lift5, 2), np.round(lift10, 2)\n",
    "    \n",
    "    ## 建模样本全量refit得到模型\n",
    "    def refit(self):\n",
    "        \n",
    "        broadcast_with_tar = self.param.get('broadcast_with_tar')\n",
    "        balanced_badrate = self.param.get(\"balanced_badrate\")\n",
    "        feas = [v for v in self.data.columns if self.data[v].dtype!='O' and v!='new_target']\n",
    "        callbacks, val_set = [], None\n",
    "        \n",
    "        X_tr, X_val, y_tr, y_val, tr_orgidx, val_orgidx = self.inference_split_data(self.data, False)\n",
    "        if balanced_badrate is not None:\n",
    "            w_tr = re_weight_by_org(y_tr, tr_orgidx, 0.5, broadcast_with_tar, balanced_badrate)\n",
    "        else:\n",
    "            w_tr = pd.Series(np.ones(len(X_tr)), index=X_tr.index)\n",
    "        train_set = lgb.Dataset(X_tr, label=y_tr, weight=w_tr)\n",
    "        \n",
    "        if 'stopping_rounds' in self.param.keys():\n",
    "            self.param.update({'num_iterations': 300})\n",
    "            callbacks.append(lgb.early_stopping(stopping_rounds=self.param.get('stopping_rounds')))\n",
    "            \n",
    "            # 更新训练集与其权重，添加验证集，早停判断标准和训练时一致为auc(不加权计算)\n",
    "            X_tr, X_val, y_tr, y_val, tr_orgidx, val_orgidx = self.inference_split_data(self.data, True)\n",
    "            \n",
    "            if balanced_badrate is not None:\n",
    "                w_tr = re_weight_by_org(y_tr, tr_orgidx, 0.5, broadcast_with_tar, balanced_badrate)\n",
    "            else:\n",
    "                w_tr = pd.Series(np.ones(len(X_tr)), index=X_tr.index)\n",
    "            \n",
    "            train_set = lgb.Dataset(X_tr, label=y_tr, weight=w_tr)\n",
    "            val_set = lgb.Dataset(X_val, label=y_val, reference=train_set)\n",
    "        \n",
    "        self.model = lgb.train(\n",
    "                          self.param,\n",
    "                          verbose_eval=0, \n",
    "                          train_set = train_set, \n",
    "                          valid_sets = val_set,\n",
    "                          callbacks = callbacks\n",
    "                         )\n",
    "        ## 保存模型\n",
    "        joblib.dump(self.model, self.store_pth+self.score_name+'.pkl')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    ## 基于建模样本单一机构做oos评估\n",
    "    def refit_cvoos_(self, param, org, X_tr, y_tr, w_tr, tr_orgidx):\n",
    "        print(org)\n",
    "        tr_idxs = set(X_tr.index)\n",
    "        oos_idx = set(tr_orgidx.get(org))\n",
    "        X_tr_, y_tr_ = X_tr.loc[list(tr_idxs-oos_idx), ], y_tr.loc[list(tr_idxs-oos_idx), ]\n",
    "        if param.get('balanced_badrate') is not None:\n",
    "            w_tr_ = w_tr.loc[list(tr_idxs-oos_idx), ]\n",
    "        else:\n",
    "            w_tr_ = pd.Series(np.ones(X_tr_.shape[0]), index=X_tr_.index)\n",
    "        train_set = lgb.Dataset(X_tr_, y_tr_, weight=w_tr_)\n",
    "        X_oos, y_oos = X_tr.loc[list(oos_idx), ], y_tr.loc[list(oos_idx), ]\n",
    "\n",
    "        cvoos_result = pd.DataFrame()\n",
    "\n",
    "        ## n-1个机构全量样本下训练\n",
    "        model = lgb.train(\n",
    "                      param,\n",
    "                      verbose_eval=0, \n",
    "                      train_set = train_set\n",
    "                     )\n",
    "\n",
    "        ## 得到oos结果\n",
    "        auc, ks, lift3, lift5, lift10 = self.inference_oos_metric(model, X_oos, y_oos)\n",
    "        nan_idx = list(X_oos[X_oos[self.child_score].isna()].index)\n",
    "        auc_, ks_, lift3_, lift5_, lift10_ = self.inference_childscore_metric(X_oos[~X_oos.index.isin(nan_idx)][self.child_score], y_oos[~y_oos.index.isin(nan_idx)])\n",
    "        cvoos_result = pd.concat([cvoos_result, pd.DataFrame({'oos': org, 'score': self.score_name, 'auc':auc, 'ks':ks, \n",
    "                                                '3%lift':lift3, '5%lift':lift5, '10%lift':lift10}, index=['0'])], axis=0)\n",
    "        cvoos_result = pd.concat([cvoos_result, pd.DataFrame({'oos': org, 'score': self.child_score, 'auc':auc_, 'ks':ks_, \n",
    "                                                '3%lift':lift3_, '5%lift':lift5_, '10%lift':lift10_}, index=['0'])], axis=0)\n",
    "        return cvoos_result\n",
    "    \n",
    "        \n",
    "    ## 目前仅支持对不设置早停的参数生成oos结果\n",
    "    def get_cvoos_result(self):\n",
    "    \n",
    "        assert 'stopping_rounds' not in self.param.keys(), \"参数中存在stopping_rounds, 无法生成oos\"\n",
    "        broadcast_with_tar = self.param.get('broadcast_with_tar')\n",
    "        balanced_badrate = self.param.get(\"balanced_badrate\")\n",
    "        \n",
    "        X_tr, _, y_tr, _, tr_orgidx, _ = self.inference_split_data(self.data, False)\n",
    "        if balanced_badrate is not None:\n",
    "            w_tr = re_weight_by_org(y_tr, tr_orgidx, 0.5, broadcast_with_tar, balanced_badrate)\n",
    "        else:\n",
    "            w_tr = pd.Series(np.ones(X_tr.shape[0]), index=X_tr.index)\n",
    "        \n",
    "        cvoos_result = pd.DataFrame()\n",
    "                # 每次1个机构做oos, 其余机构做trainset\n",
    "        for org in self.data.new_org.unique():\n",
    "            \n",
    "            tr_idxs = set(X_tr.index)\n",
    "            oos_idx = set(tr_orgidx.get(org))\n",
    "            X_tr_, y_tr_ = X_tr.loc[list(tr_idxs-oos_idx), ], y_tr.loc[list(tr_idxs-oos_idx), ]\n",
    "            if balanced_badrate is not None:\n",
    "                w_tr_ = w_tr.loc[list(tr_idxs-oos_idx), ]\n",
    "            else:\n",
    "                w_tr_ = pd.Series(np.ones(len(X_tr_)), index=X_tr_.index)\n",
    "            train_set = lgb.Dataset(X_tr_, y_tr_, weight=w_tr_)\n",
    "            X_oos, y_oos = X_tr.loc[list(oos_idx), ], y_tr.loc[list(oos_idx), ]\n",
    "            \n",
    "            ## n-1个机构全量样本下训练\n",
    "            model = lgb.train(\n",
    "                          self.param,\n",
    "                          verbose_eval=0, \n",
    "                          train_set = train_set\n",
    "                         )\n",
    "            \n",
    "            ## 得到oos结果\n",
    "            auc, ks, lift3, lift5, lift10 = self.inference_oos_metric(model, X_oos, y_oos)\n",
    "            nan_idx = list(X_oos[X_oos[self.child_score].isna()].index)\n",
    "            auc_, ks_, lift3_, lift5_, lift10_ = self.inference_childscore_metric(X_oos[~X_oos.index.isin(nan_idx)][self.child_score], y_oos[~y_oos.index.isin(nan_idx)])\n",
    "            cvoos_result = pd.concat([cvoos_result, pd.DataFrame({'oos': org, 'score': self.score_name, 'auc':auc, 'ks':ks, \n",
    "                                                    '3%lift':lift3, '5%lift':lift5, '10%lift':lift10}, index=['0'])], axis=0)\n",
    "            cvoos_result = pd.concat([cvoos_result, pd.DataFrame({'oos': org, 'score': self.child_score, 'auc':auc_, 'ks':ks_, \n",
    "                                                    '3%lift':lift3_, '5%lift':lift5_, '10%lift':lift10_}, index=['0'])], axis=0)\n",
    "            \n",
    "        return cvoos_result\n",
    "        \n",
    "#         # 开启10个进程池运行lgb\n",
    "#         tasks = [(org) for org in self.tr_orgidx.keys()]\n",
    "#         results = []\n",
    "#         pool = multiprocessing.Pool()\n",
    "#         for task in tasks:\n",
    "#             results.append(pool.apply_async(self.refit_cvoos_, task))\n",
    "#         pool.join()\n",
    "#         pool.close()\n",
    "#         with Pool(9) as pool:\n",
    "#             records = pool.starmap(self.refit_cvoos_, tasks)\n",
    "        \n",
    "#         return cvoos_result\n",
    "    \n",
    "    def fixedbins_results(self, data, col, n_bins):\n",
    "        \n",
    "        tmp = data[[col, 'new_target']].copy()\n",
    "        \n",
    "        combiner = toad.transform.Combiner()\n",
    "        combiner.fit(tmp, y=tmp['new_target'], method='quantile', n_bins=n_bins)\n",
    "        bin_edges = combiner.export().get(col)\n",
    "        min_edge = np.min(tmp[col])\n",
    "        max_edge = np.max(tmp[col])\n",
    "        \n",
    "        def _bin_to_interval(bin_label, bin_edges):\n",
    "            if pd.isna(bin_label):\n",
    "                return 'NaN'\n",
    "            bin_label = int(bin_label)\n",
    "            if bin_label == 0:\n",
    "                return f\"[{min_edge}, {bin_edges[0]})\"\n",
    "            elif bin_label == len(bin_edges):\n",
    "                return f\"[{bin_edges[-1]}, {max_edge}]\"\n",
    "            else:\n",
    "                return f\"[{bin_edges[bin_label-1]}, {bin_edges[bin_label]})\"\n",
    "\n",
    "        tmp['bin_'] = combiner.transform(tmp[[col]])[[col]]\n",
    "        tmp['bin'] = tmp['bin_'].apply(lambda x: _bin_to_interval(x, bin_edges))\n",
    "        tmp['bin'] = tmp['bin'].astype(str)\n",
    "        \n",
    "        res = tmp.groupby(['bin']).apply(\n",
    "            lambda x: pd.Series({\n",
    "                '变量名': col,\n",
    "                '分箱': x['bin'].iloc[0],\n",
    "                '命中率': round(len(x)*1.0 / tmp.shape[0], 4),\n",
    "                '坏样率': round(np.mean(x['new_target']), 4),\n",
    "                '分箱正样本数': x['new_target'].sum(),\n",
    "                '分箱负样本数': len(x) - x['new_target'].sum(),\n",
    "                '分箱ks': round(toad.metrics.KS(x[col], x['new_target']), 4),\n",
    "                 ## 默认使用等频5分箱计算每个x的iv值\n",
    "                '分箱iv': calculate_iv(x[[col]], x['new_target'], 'quantile', 5)[0],\n",
    "                '分箱lift': round(x['new_target'].mean() / tmp['new_target'].mean(), 2),\n",
    "                '召回率': round(sum(x['new_target']) * 1.0 / sum(tmp['new_target']) ,4),\n",
    "                '特异度': round((len(x)-sum(x['new_target'])) * 1.0 / (tmp.shape[0] - sum(tmp['new_target'])), 4)\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        res = pd.DataFrame(res).reset_index(drop=True).sort_values(by=['分箱'])\n",
    "        \n",
    "        res['分箱iv'].replace({np.inf: 0}, inplace=True)\n",
    "        res['总IV'] = sum(res['分箱iv'])\n",
    "        res['累计正样本数'], res['累计负样本数'] =  res['分箱正样本数'].cumsum(), res['分箱负样本数'].cumsum()\n",
    "        res['累计样本数'] = res['累计正样本数'] + res['累计负样本数']\n",
    "        res['累积命中率'] = res['命中率'].cumsum()\n",
    "        res['累积提升度'] = round((res['累计正样本数']/res['累计样本数'])*1.0/tmp['new_target'].mean(), 4)\n",
    "        res['累积召回率'] = round(res['累计正样本数']*1.0/tmp['new_target'].sum(), 4)\n",
    "        res['累积特异度'] = round(res['累计负样本数']*1.0/(tmp.shape[0]-tmp['new_target'].sum()), 4)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \n",
    "        if self.model is None:\n",
    "            print(\"step 1 使用data拟合模型\")\n",
    "            self.refit()\n",
    "        else:\n",
    "            print(\"step 1 加载输入的模型\")\n",
    "        \n",
    "        try:\n",
    "            Path(self.store_pth+\"train logs/auc\").mkdir(parents=True, exist_ok=True)\n",
    "            Path(self.store_pth+\"train logs/ks\").mkdir(parents=True, exist_ok=True)\n",
    "            Path(self.store_pth+\"train logs/lift\").mkdir(parents=True, exist_ok=True)\n",
    "            Path(self.store_pth+\"train logs/trend\").mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"train log目录已就绪\")\n",
    "        except Exception as e:\n",
    "            print(f'{e} train log目录生成失败')\n",
    "        \n",
    "        ## 得到train中每个机构做oos结果\n",
    "        print(\"step2 计算cv oos结果\")\n",
    "        cv_trainoos_result = self.get_cvoos_result()\n",
    "        \n",
    "        ## 得到真正的oos set结果\n",
    "        cv_oos_result = pd.DataFrame()\n",
    "        for org in self.oos_data.new_org.unique():\n",
    "            oos_data_ = self.oos_data[self.oos_data.new_org==org].copy()\n",
    "            auc, ks, lift3, lift5, lift10 = self.inference_oos_metric(self.model, oos_data_[self.model.feature_name()], oos_data_['new_target'])\n",
    "            cv_oos_result = pd.concat([cv_oos_result, pd.DataFrame({'oos': org, 'score':self.score_name, 'auc':auc, 'ks':ks, '3%lift':lift3, \n",
    "                                                    '5%lift':lift5, '10%lift':lift10}, index=['0'])], axis=0)\n",
    "            ## 计算子分\n",
    "            nan_idx = list(oos_data_[oos_data_[self.child_score].isna()].index)\n",
    "            auc_, ks_, lift3_, lift5_, lift10_ = self.inference_childscore_metric(oos_data_[~oos_data_.index.isin(nan_idx)][self.child_score], oos_data_[~oos_data_.index.isin(nan_idx)]['new_target'])\n",
    "            cv_oos_result = pd.concat([cv_oos_result, pd.DataFrame({'oos': org, 'score':self.child_score, 'auc':auc_, 'ks':ks_, '3%lift':lift3_, \n",
    "                                                    '5%lift':lift5_, '10%lift':lift10_}, index=['0'])], axis=0)\n",
    "        \n",
    "        cv_oos_result = pd.concat([cv_trainoos_result, cv_oos_result], axis=0)\n",
    "        \n",
    "        \n",
    "        ## 保存gain值图\n",
    "        self.feas_gain = pd.DataFrame(list(dict(zip(self.model.feature_name(), self.model.feature_importance(importance_type='gain'))).items()))\n",
    "        self.feas_gain = self.feas_gain.sort_values(by=1, ascending=False)\n",
    "        self.feas_gain['2'] = round(self.feas_gain[1] / sum(self.feas_gain[1]), 2)\n",
    "        self.feas_gain.columns = ['变量', 'gain', 'gain值占比']\n",
    "        \n",
    "        ##绘制shap图\n",
    "        #explain = shap.TreeExplainer(self.model)\n",
    "        #shap_values = explain.shap_values(self.oot_data[self.model.booster_.feature_name()])\n",
    "        #shap.summary_plot(shap_values[1], self.oot_data[self.model.booster_.feature_name()], max_display=20)\n",
    "        #plt.savefig(self.store_pth+\"shap_summary.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "        #plt.close()\n",
    "        \n",
    "        self.data[self.score_name] = np.round(self.model.predict(self.data[self.model.feature_name()]), 3)\n",
    "        self.oos_data[self.score_name] = np.round(self.model.predict(self.oos_data[self.model.feature_name()]), 3)\n",
    "        \n",
    "        if self.score_transform_func is not None:\n",
    "            self.data[self.score_name]  = self.score_transform_func(self.data[self.score_name])\n",
    "            self.oos_data[self.score_name]  = self.score_transform_func(self.oos_data[self.score_name])\n",
    "        \n",
    "        psi_oos = calculate_psi(self.data[self.score_name], self.oos_data[self.score_name])\n",
    "        print(f\"psi_oos是{psi_oos}\")\n",
    "        \n",
    "        ## 绘制箱线图\n",
    "        fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
    "        sns.boxplot(y='new_org', x=self.child_score, data=self.data, ax=ax1)\n",
    "        fig1.savefig(self.store_pth+\"train logs/trend/子分箱线图.jpg\")\n",
    "        plt.close()\n",
    "        \n",
    "        fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "        sns.boxplot(y='new_org', x=self.score_name, data=self.data, ax=ax2)\n",
    "        fig2.savefig(self.store_pth+\"train logs/trend/评分箱线图.jpg\")\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"step 3 计算模型得分&子分等频10分箱\")\n",
    "        bins_results = self.fixedbins_results(self.data, self.score_name, 10)\n",
    "        bins_results_oos = self.fixedbins_results(self.oos_data, self.score_name, 10)\n",
    "        bins_results_childscore = self.fixedbins_results(self.data, self.child_score, 10)\n",
    "        bins_results_oos_childscore = self.fixedbins_results(self.oos_data, self.child_score, 10)\n",
    "        \n",
    "        print(\"step 4 分机构计算模型得分&子分等频10分箱\")\n",
    "        bins_results_org = pd.DataFrame()\n",
    "        for org in data.new_org.unique():\n",
    "            tmp_data = data[data.new_org==org].copy()\n",
    "            try:\n",
    "                bins_results_org_ = self.fixedbins_results(tmp_data, self.score_name, 10)\n",
    "            except:\n",
    "                print(f\"{org}评分分为10箱中有部分箱全为0|1, 改为3分箱\")\n",
    "                bins_results_org_ = self.fixedbins_results(tmp_data, self.score_name, 3)\n",
    "                \n",
    "            bins_results_org_.insert(0, '机构', org)\n",
    "            bins_results_org = pd.concat([bins_results_org, bins_results_org_], axis=0)\n",
    "            \n",
    "            try:\n",
    "                bins_results_org_ = self.fixedbins_results(tmp_data, self.child_score, 10)\n",
    "            except:\n",
    "                print(f\"{org}子分分为10箱中有部分箱全为0|1, 改为3分箱\")\n",
    "                bins_results_org_ = self.fixedbins_results(tmp_data, self.child_score, 3)\n",
    "            bins_results_org_.insert(0, '机构', org)\n",
    "            bins_results_org = pd.concat([bins_results_org, bins_results_org_], axis=0)\n",
    "        \n",
    "        \n",
    "        print(\"step 5 生成评分趋势图\")\n",
    "        ## 保存data数据下得分10分箱子坏样率趋势图\n",
    "        iv, edge_dict, _ = calculate_iv(self.data[self.score_name], self.data['new_target'], 'quantile', 10)\n",
    "        iv, woe, fig = trend_detect(self.data[self.score_name], self.data['new_target'], edge_dict)\n",
    "        fig.savefig(self.store_pth+\"train logs/trend/建模样本得分趋势图.jpg\")\n",
    "        ## 保存oos_data数据下得分10分箱子坏样率趋势图\n",
    "        iv, edge_dict, _ = calculate_iv(self.oos_data[self.score_name], self.oos_data['new_target'], 'quantile', 10)\n",
    "        iv, woe, fig = trend_detect(self.oos_data[self.score_name], self.oos_data['new_target'], edge_dict)\n",
    "        fig.savefig(self.store_pth+\"train logs/trend/oos得分趋势图.jpg\")\n",
    "        \n",
    "        print(\"step 6 生成训练过程图\")\n",
    "        for org in self.results.org.unique():\n",
    "    \n",
    "            results_auc = self.results[(self.results.org==org) & (self.results.idx=='auc')]\n",
    "            results_auc_w = self.results[(self.results.org==org) & (self.results.idx=='auc_w')]\n",
    "            results_ks = self.results[(self.results.org==org) & (self.results.idx=='ks')]\n",
    "            results_ks_w = self.results[(self.results.org==org) & (self.results.idx=='ks_w')]\n",
    "            results_5lift = self.results[(self.results.org==org) & (self.results.idx=='5%lift')]\n",
    "            results_10lift = self.results[(self.results.org==org) & (self.results.idx=='10%lift')]\n",
    "\n",
    "            x = np.arange(len(ast.literal_eval(results_auc.train.iloc[0])))\n",
    "            tr_auc, val_auc, oos_auc = ast.literal_eval(results_auc.train.iloc[0]), ast.literal_eval(results_auc.val.iloc[0]), ast.literal_eval(results_auc.oos.iloc[0])\n",
    "            tr_auc_w, val_auc_w, oos_auc_w = ast.literal_eval(results_auc_w.train.iloc[0]), ast.literal_eval(results_auc_w.val.iloc[0]), ast.literal_eval(results_auc_w.oos.iloc[0])\n",
    "            df = pd.DataFrame({\n",
    "                    'iteration': np.tile(x, 6),\n",
    "                    'value': np.concatenate([tr_auc, val_auc, oos_auc, tr_auc_w, val_auc_w, oos_auc_w]),\n",
    "                    'type': ['train']*len(x) + ['val']*len(x) + ['oos']*len(x) + ['train_w']*len(x) + ['val_w']*len(x) + ['oos_w']*len(x),\n",
    "                    'weight': ['no']*3*len(x) + ['yes']*3*len(x)\n",
    "                })\n",
    "\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "            sns.lineplot(\n",
    "                data=df[df.weight=='no'],\n",
    "                x='iteration',\n",
    "                y='value',\n",
    "                hue='type',\n",
    "                ax=ax1\n",
    "            )\n",
    "            ax1.set_title(f\"auc, 贷外是{org}\")\n",
    "\n",
    "            sns.lineplot(\n",
    "                data=df[df.weight=='yes'],\n",
    "                x='iteration',\n",
    "                y='value',\n",
    "                hue='type',\n",
    "                ax=ax2\n",
    "            )\n",
    "            ax2.set_title(f\"加权auc, 贷外是{org}\")\n",
    "            plt.tight_layout()\n",
    "            fig.savefig(f'{self.store_pth}train logs/auc/auc_{org}.jpg')\n",
    "\n",
    "            ## 绘制ks图\n",
    "            tr_ks, val_ks, oos_ks = ast.literal_eval(results_ks.train.iloc[0]), ast.literal_eval(results_ks.val.iloc[0]), ast.literal_eval(results_ks.oos.iloc[0])\n",
    "            tr_ks_w, val_ks_w, oos_ks_w = ast.literal_eval(results_ks_w.train.iloc[0]), ast.literal_eval(results_ks_w.val.iloc[0]), ast.literal_eval(results_ks_w.oos.iloc[0])\n",
    "            df = pd.DataFrame({\n",
    "                    'iteration': np.tile(x, 6),\n",
    "                    'value': np.concatenate([tr_ks, val_ks, oos_ks, tr_ks_w, val_ks_w, oos_ks_w]),\n",
    "                    'type': ['train']*len(x) + ['val']*len(x) + ['oos']*len(x) + ['train_w']*len(x) + ['val_w']*len(x) + ['oos_w']*len(x),\n",
    "                    'weight': ['no']*3*len(x) + ['yes']*3*len(x)\n",
    "                })\n",
    "\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "            sns.lineplot(\n",
    "                data=df[df.weight=='no'],\n",
    "                x='iteration',\n",
    "                y='value',\n",
    "                hue='type',\n",
    "                ax=ax1\n",
    "            )\n",
    "            ax1.set_title(f\"ks, 贷外是{org}\")\n",
    "\n",
    "            sns.lineplot(\n",
    "                data=df[df.weight=='yes'],\n",
    "                x='iteration',\n",
    "                y='value',\n",
    "                hue='type',\n",
    "                ax=ax2\n",
    "            )\n",
    "            ax2.set_title(f\"加权ks, 贷外是{org}\")\n",
    "            plt.tight_layout()\n",
    "            fig.savefig(f'{self.store_pth}train logs/ks/ks_{org}.jpg')\n",
    "\n",
    "            ## 绘制lift\n",
    "            tr_5lift, val_5lift, oos_5lift = ast.literal_eval(results_5lift.train.iloc[0]), ast.literal_eval(results_5lift.val.iloc[0]), ast.literal_eval(results_5lift.oos.iloc[0])\n",
    "            tr_10lift, val_10lift, oos_10lift = ast.literal_eval(results_10lift.train.iloc[0]), ast.literal_eval(results_10lift.val.iloc[0]), ast.literal_eval(results_10lift.oos.iloc[0])\n",
    "            df = pd.DataFrame({\n",
    "                    'iteration': np.tile(x, 6),\n",
    "                    'value': np.concatenate([tr_5lift, val_5lift, oos_5lift, tr_10lift, val_10lift, oos_10lift]),\n",
    "                    'type': ['train']*len(x) + ['val']*len(x) + ['oos']*len(x) + ['train']*len(x) + ['val']*len(x) + ['oos']*len(x),\n",
    "                    'weight': ['no']*3*len(x) + ['yes']*3*len(x)\n",
    "                })\n",
    "\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "            sns.lineplot(\n",
    "                data=df[df.weight=='no'],\n",
    "                x='iteration',\n",
    "                y='value',\n",
    "                hue='type',\n",
    "                ax=ax1\n",
    "            )\n",
    "            ax1.set_title(f\"5%lift, 贷外是{org}\")\n",
    "\n",
    "            sns.lineplot(\n",
    "                data=df[df.weight=='yes'],\n",
    "                x='iteration',\n",
    "                y='value',\n",
    "                hue='type',\n",
    "                ax=ax2\n",
    "            )\n",
    "            ax2.set_title(f\"10%lift, 贷外是{org}\")\n",
    "            plt.tight_layout()\n",
    "            fig.savefig(f'{self.store_pth}train logs/lift/lift_{org}.jpg')\n",
    "        \n",
    "        print(\"step 7 生成模型报告中\")\n",
    "        writer = pd.ExcelWriter(self.store_pth+self.score_name+'.xlsx', engine='xlsxwriter')\n",
    "        ## 表一\n",
    "        self.dataset_statis.to_excel(writer, sheet_name='全量样本分机构概览', index=False)\n",
    "        ## 表二\n",
    "        cv_oos_result.to_excel(writer, sheet_name='分机构贷外效果', index=False)\n",
    "        ## 表三\n",
    "        bins_results.to_excel(writer, sheet_name='建模样本评分10分箱', index=False)\n",
    "        ## 表四\n",
    "        bins_results_oos.to_excel(writer, sheet_name='oos评分10分箱', index=False)\n",
    "        ## 表五\n",
    "        bins_results_childscore.to_excel(writer, sheet_name='建模样本子分10分箱', index=False)\n",
    "        ## 表六\n",
    "        bins_results_oos_childscore.to_excel(writer, sheet_name='oos子分10分箱', index=False)\n",
    "        ## 表七\n",
    "        bins_results_org.to_excel(writer, sheet_name='分机构评分&子分10分箱', index=False)\n",
    "        ## 表八\n",
    "        self.feas_gain.to_excel(writer, sheet_name='模型gain值排序', index=False)\n",
    "        ## 表九\n",
    "        self.param['randn'] = self.randn\n",
    "#         self.param['train orgs'] = list(self.data.new_org.unique())\n",
    "#         self.param['oos orgs'] = list(self.oos_data.new_org.unique())\n",
    "        pd.DataFrame(self.param, index=['0']).to_excel(writer, sheet_name='模型参数', index=False)\n",
    "\n",
    "        writer.save()\n",
    "        writer.close()\n",
    "        \n",
    "        print(\"已完成模型报告\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'balanced_badrate': None,\n",
    "         'boosting_type': 'gbdt',\n",
    "         'broadcast_with_tar': False,\n",
    "         'colsample_bytree': 0.8,\n",
    "         'learning_rate': 0.05,\n",
    "         'max_bin': 120,\n",
    "         'max_depth': 5,\n",
    "         'metric': 'auc',\n",
    "         'min_child_samples': 60,\n",
    "         'min_child_weight': 35,\n",
    "         'num_iterations': 275,\n",
    "         'num_threads': 2,\n",
    "         'objective': 'binary',\n",
    "         'reg_alpha': 3.0,\n",
    "         'reg_lambda': 2.0\n",
    "}\n",
    "\n",
    "results = pd.read_csv(\"tx process wo balanced/results_new.csv\")\n",
    "dataset_statis = pd.read_csv(\"tx process wo balanced/datasetStatis.csv\")\n",
    "randn = 42\n",
    "store_pth = \"tx process wo balanced/\"\n",
    "\n",
    "kwargs = {'data':data, 'oos_data':oos_data, 'param': param, 'randn': randn,\n",
    "          'results': results, 'dataset_statis': dataset_statis,'child_score':'model_risk_v8_gtongyong_score',\n",
    "         'score_name':'tx_mixedScore_wo_balanced', 'store_pth':store_pth ,'score_transform_func':None, \"model\": None}\n",
    "inference = Inference(**kwargs)\n",
    "inference.generate_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "\n",
    "# low_threshold, high_threshold = None, None\n",
    "# numeric_features = [f for f in list(data.columns) if data[f].dtype in [int, float] and f!='new_target']\n",
    "# stds = data[numeric_features].std(axis=0, skipna=True)\n",
    "# means = data[numeric_features].mean(axis=0, skipna=True)\n",
    "# cvs = (stds / means).abs()\n",
    "# xlim = [0, cvs.max()]\n",
    "# display(xlim)\n",
    "# if low_threshold is not None:\n",
    "#     cvs = cvs[cvs > low_threshold]\n",
    "#     xlim[0] = low_threshold\n",
    "# if high_threshold is not None:\n",
    "#     cvs = cvs[cvs < high_threshold]\n",
    "#     xlim[1] = high_threshold\n",
    "\n",
    "# print(\"Stats about the range of the coefficient of variation across all features\")\n",
    "# print(cvs.describe())\n",
    "# cvs.plot(kind='hist', xlim=tuple(xlim), logx=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_nunique_ratio(df, drop_nonumeric=True):\n",
    "    if drop_nonumeric:\n",
    "        numeric_columns = df.head()._get_numeric_data().columns\n",
    "        nonnumeric_columns = [f for f in df if f not in numeric_columns]\n",
    "        df = df[nonnumeric_columns]\n",
    "    else:\n",
    "        nonnumeric_columns = df.columns\n",
    "\n",
    "    nunique = df.nunique(axis=0, dropna=True)\n",
    "    total = df[nonnumeric_columns].count(axis=0)\n",
    "    return nunique / total\n",
    "\n",
    "a = pd.DataFrame(get_categorical_nunique_ratio(data, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
